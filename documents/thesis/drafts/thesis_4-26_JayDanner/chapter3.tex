\clearpage
\chapter{Methodology} \label{chapter:methodology}

This chapter contains the research methodology employed while implementing a proof-of-concept of an adaptive shadow removal model. Our methodology is divided into two components: algorithm assessment in the context of shadow removal in diverse environments, and the creation of the adaptive model for Physical shadow removal, one of the previously assessed algorithms.

We first assess the previously catalogued algorithms (Chromacity, Physical, Geometry, SRT, and LRT removal) for sensitivity to both environmental and parametric change. We then perform a similar sensitivity analysis upon the parameters utilized by the algorithms. These assessments are performed qualitatively to determine the feasibility of an adaptive model being created for each algorithm.

From the sensitivity assessments, we demonstrate the creation of an adaptive model for physical shadow removal's \textit{coneR1} parameter, which bounds the brightness deviation used to distinguish shadow pixels from foreground. We consider various environmental properties and evaluate their correlations to the selected algorithm parameter. Potential indirect correlative factors are detailed in sections \ref{section:brightnessmodels} and \ref{section:lowcSIFT}. Finally, methodology concerning assembling the adaptive model follows in section \ref{section:model}.

\section{Algorithm Assessment}

We qualitatively assess several leading shadow removal algorithms (Chromacity, Physical, Geometry, SRT, and LRT removal) for their sensitivity to different environmental properties. Environmental properties are identified across diverse datasets, examples including shadow darkness, shadow directionality, and color saturation. Environmental property shifts, such as illumination changes, are also observed in multiple datasets.

In addition to analyzing the algorithms' sensitivity to environmental change, we similarly assess the algorithms' sensitivity to parametric shift, i.e., algorithm performance is qualitatively assessed according to the algorithm's dependence on one or more curated parameter values. Analysis and assessment of the previously presented shadow removal algorithms is conducted using a series of graphical tools in conjunction with ground truths identifying shadow regions across eight different datasets with seven unique environments.

\subsection{Data Collection}  \label{section:datacollection}

This section details the datasets used to conduct our assessment, as well as tools developed for analyzing the sensitivity of algorithms and parameters. The tools and techniques covered allow for both qualitative and quantitative assessment.

\subsubsection{Datasets}

A total of eight datasets were chosen for this assessment, and are summarized in Table \ref{table:datasets}. Datasets 2 - 8 (denoted by the prefix ``aton\_'') are provided under the Computer Vision and Robotics Research Laboratory (CVRR) in association with the University of San Diego \cite{cvrr}. The datasets were cataloged with the Autonomous Agents for On-Scene Networked Incident Management (ATON) project \cite{aton2002}. These datasets represent diverse environments, including both indoor and outdoor scenes, a range of shadow intensity, and images of varying saturation.

In addition, two datasets were included from the Performance Evaluation of Tracking and Surveillance (PETS) project \cite{pets2001}. In contrast to the ATON datasets, these sequences were explicitly chosen because they possess subsequences featuring rapid illumination changes, due to a shift in weather conditions and cloud cover. These environmental changes darken, lighten, and blend shadows in accordance with a background model. Figure \ref{fig:pets2illum} demonstrates the illumination changes within the PETS datasets.

All eight datasets were manually segmented to form ground truths that identify shadow regions (gray) within foreground objects (white). Figure \ref{fig:datasetsgt} illustrates the ground truths of dataset frames.


\begin{table}
\centering
\begin{tabular}{ |c|c|c|c| }
	\hline
	\textbf{Name} & \textbf{Resolution} & \textbf{Sequence Length} & \textbf{\# GT Frames} \\
	\hline
	\hline
	PETS1 & 768x576 & 175 & 12 \\
	\hline
	PETS2 & 768x576 & 192 & 11 \\
	\hline
	aton\_highway1 & 320x240 & 440 & 8 \\
	\hline 
	aton\_highway3 & 320x240 & 2227 & 7 \\ 
	\hline
	aton\_room & 320x240 & 300 & 22 \\ 
	\hline
	aton\_campus & 320x240 & 1179 & 53 \\ 
	\hline
	aton\_hallway & 320x240 & 1800 & 13 \\
	\hline
	aton\_lab & 320x240 & 887 & 14 \\ 
	\hline
\end{tabular}
\caption{Dataset information.}
\label{table:datasets}
\end{table}

\begin{figure}
\centering
\begin{subfigure}{.48\linewidth}
  \includegraphics[width=1\linewidth]{figures/PETS2_highv.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.48\linewidth}
  \includegraphics[width=1\linewidth]{figures/PETS2_lowv.jpg}
  \caption{}
\end{subfigure}
\caption{PETS2 experiencing both high illumination (a) and low illumination (b) (due to cloud cover).}
\label{fig:pets2illum}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_0045.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_0151.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/hallway_0164.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_gt_0045.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_gt_0151.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.3\linewidth}
  \includegraphics[width=1\linewidth]{figures/hallway_gt_0164.jpg}
  \caption{}
\end{subfigure}

\caption{Datasets collected for ATON: (a) aton\_highway1, (b) aton\_lab, and (c) aton\_hallway.}
\label{fig:datasetsgt}
\end{figure}

\FloatBarrier
\subsubsection{Assessment Tools}

The first stage of our methodology is an exploration of the sensitivity of shadow removal algorithms to environmental and parametric change. We developed tools to assist intuition regarding an algorithm's performance, capable of allowing a user to manually tune an algorithm parameter, and observe its effect on performance.

To better understand an algorithm's dependence on a parameter, a framework was required to rapidly modify the parameter in question. The initial implementation of the shadow removal methods, courtesy of Sanin et al. \cite{shadowssourceforge}, contained relevant parameters hard-coded into the algorithms. Each algorithm therefore required re-compilation to bring any modifications to fruition. This shortcoming was rectified by extracting each parameter and organizing them into an \textit{.ini} file. By utilizing username \textit{brofield}'s SimpleINI architecture, hosted on Github \cite{simpleini}, parameters can be either set extemporaneously or altered by an external script. An example SimpleIni .ini file is shown in Figure \ref{fig:simpleini}.

\begin{figure}
  \centering
  \includegraphics[width=.5\linewidth]{figures/simpleini.png}
  \caption{An example SimpleINI file. Parameters taken from this file are used to adjust values in real-time.}
  \label{fig:simpleini}
\end{figure}

Graphical tools (GUI) were then developed to rapidly assess and visualize a parameter's affect on shadow removal. Each shadow removal method was modified to accept arbitrary parameter values in real-time from the .ini file. These parameters were then tied to graphical sliders (from OpenCV's highgui library \cite{opencv}) dictating their range and value. Any numerical parameter that has the potential to modify the Detection or Discrimination rates of a shadow removal method is included in the GUI. An example of the GUI is seen in Figure \ref{fig:guitools}. The process in which the GUI modifies and displays parameters is illustrated in Figure \ref{fig:guimodel}.

\begin{figure}
  \centering
  \includegraphics[width=.45\linewidth]{figures/geo_highway1_default.png}

\caption{GUI tools created using OpenCV, displaying Geometry shadow removal.}
\label{fig:guitools}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/gui_model.png}
  \caption{The GUI takes user input through sliders, updates values in a .ini file, which are used to produce a new output image indicating shadow versus foreground pixels with the corresponding detection and discrimination rates. The GUI then visualizes this output.}
  \label{fig:guimodel}
\end{figure}

The display is updated in real-time with both a visual representation of detected shadow pixels (in gray), and a quantified display of both the exact detection and discrimination rates, computed based on the ground truth. Leveraging human perception, the graphical tools enabled rapid evaluation of the sensitivity of an algorithm to changes in parameter values within specific scenes.

\FloatBarrier
%%%
\subsubsection{Determining Optimal Parameter Values}
%%%

Modifying the algorithms to accept arbitrary parameter value settings at each frame enables us to create batch jobs to systematically vary parameter values across multiple runs. This allows us to meticulously sweep a given parameter ($pr$) through a specified range and record its corresponding affect upon detection and discrimination rates. Figure \ref{fig:guiiterate} illustrates the data collection process. The calculated detection and discrimination responses to change in $pr$ are recorded as the functions $\eta(pr)$ and $\xi(pr)$, respectively. Both $\eta(pr)$ and $\xi(pr)$ are determined for one frame. Figure \ref{fig:campusddscore}(a) demonstrates an example of these responses.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/gui_iterate.png}
  \caption{For one frame, a parameter is systematically iterated to provide detection/discrimination results for each possible parameter value.}
  \label{fig:guiiterate}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{.8\linewidth}
  	\includegraphics[width=1\linewidth]{figures/campus_dd.jpg}
  \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.8\linewidth}
  	\includegraphics[width=1\linewidth]{figures/campus_score.jpg}
  \caption{}
  \end{subfigure}
\caption{(a) Detection (blue) and Discrimination (orange) rates are charted against the iterated parameter value $pr$ (x-axis). For each parameter value, detection and discrimination are summed to produce a removal efficacy score $\Xi(pr)$. The global maximum of this value is the optimal parameter value ($pr$*).}
\label{fig:campusddscore}
\end{figure}

We also utilize the iterative process to determine the optimal value of $pr$ for each frame in a dataset, i.e., which $pr$ value yields the greatest improvements in detection and discrimination. This optimal value is denoted as $pr$*. The detection and discrimination rates are summed, resulting in a shadow removal score $\Xi(pr)$ (Eqn. \ref{eqn:score}), which quantifies the efficacy of the shadow removal algorithm given a certain $pr$ value. Figure \ref{fig:campusddscore}(b) illustrates the score calculated using the responses found in Figure \ref{fig:campusddscore}(a). We define the optimal value $pr$* as the global maximum of $\Xi(pr)$ (Eqn. \ref{eqn:optscore}).

\begin{equation}
\Xi(pr) = \eta(pr) + \xi(pr)
\label{eqn:score}
\end{equation}

\begin{equation}
pr\emph{*} = max(\Xi(pr))
\label{eqn:optscore}
\end{equation}

To assess the performance of algorithms throughout a dataset, $pr$* is calculated for each frame. The $pr$* values are used to determine degrees of correlation for an environment, and set the foundation for a general model of arbitrary shadow removal improvement.

\FloatBarrier
\subsection{Algorithm Selection Strategy} \label{section:selectalgorithm}

Using the tools detailed in section \ref{section:datacollection}, we perform a brief qualitative assessment of the shadow removal algorithms' sensitivity. Given the diversity of shadow removal methods, a wide array of environmental factors may potentially influence an even wider array of algorithmic parameters. The scope of this research has therefore been focused on identifying a suitable algorithm to assess, linking a parameter (intrinsic to this algorithm) to unique environmental properties or changes, and developing a model for real-time parameter adaptation based on correlations found.
%and begin the construction of a general model of shadow removal improvement given analogous parameters.

%Identifying an appropriate algorithm for assessment means highlighting a shadow removal method that demonstrates consistently reasonable sensitivity to changes in environmental characteristics. Simultaneously, a candidate for deeper analysis must demonstrate consistency in its detection/discrimination integrity throughout most frames of a dataset, with few sizable deviations. If the study were not constrained in this manner, the motivation of this research quickly becomes redundant, as the sheer amount of interdependent variables brings the problem to levels generally reserved for unsupervised clustering and other big-data approaches. As this study means to link qualitative environmental characteristics to quantitative tangible improvement, this approach would detour research into a different realm entirely.

Candidate shadow removal algorithms are evaluated according to the correlation between their efficacy (in terms of detection and discrimination rates) over time, and qualitative observations attributed to a dataset; e.g., the PETS1 dataset experiences a large illumination change midway through its sampling, which is taken into account when searching for trends among the accuracy of a candidate method. Figure \ref{fig:selectinganalgorithm} demonstrates shadow detection rates using the PETS1 dataset, for each shadow removal algorithm.

\begin{sidewaysfigure}
\centering
\begin{subfigure}{.32\linewidth}
  \includegraphics[width=1\linewidth]{figures/selectinganalgorithm_chromacity.jpg}
  \caption{Chromacity removal.}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \includegraphics[width=1\linewidth]{figures/selectinganalgorithm_physical.jpg}
  \caption{Physical removal.}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \includegraphics[width=1\linewidth]{figures/selectinganalgorithm_geometry.jpg}
  \caption{Geometry removal.}
\end{subfigure}
\hfill
\begin{subfigure}{.32\linewidth}
  \includegraphics[width=1\linewidth]{figures/selectinganalgorithm_srt.jpg}
  \caption{Small-Region Texture removal.}
\end{subfigure}
%\hfill
\begin{subfigure}{.32\linewidth}
  \includegraphics[width=1\linewidth]{figures/selectinganalgorithm_lrt.jpg}
  \caption{Large-Region Texture removal.}
\end{subfigure}

\caption{Shadow detection ($\eta$)(orange) for each algorithm, run on the PETS1 dataset during an illumination change. Overall brightness of a scene is shown in gray.}
\label{fig:selectinganalgorithm}
\end{sidewaysfigure}

\FloatBarrier
\subsubsection{Evaluation of Methods}

We evaluate popular shadow removal methods introduced in section \ref{section:removalmethods}: Chromacity, Physical, Geometry, SRT, and LRT. This section describes our rationale for using Physical shadow removal as part of our proof-of-concept, implemented starting in section \ref{section:selectparameter}. Our evaluation considers the tunable parameters of an algorithm, dependence on environmental properties and content, and sensitivity of relevant parameters.

A parameter's sensitivity is evaluated by viewing its detection and discrimination responses ($\eta(pr)$, $\xi(pr)$). Figure \ref{fig:paramsensitivity} illustrates what is considered a sensitive parameter. A parameter is considered sensitive when a narrow range of parameter values affect detection and discrimination rates disproportionately. This sensitivity causes problems for an adaptive model, as the range of potential optimal parameter values ($pr$*) is too small to correlate with environmental properties. 

\begin{figure}
\centering
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/sensitive_param.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_dd.jpg}
  \caption{}
\end{subfigure}

\caption{(a) depicts a sensitive parameter, while (b) represents a typical parameter.}
\label{fig:paramsensitivity}
\end{figure}

%Both Geometry removal and SRT removal were shown to behave highly erratically from frame to frame even within an environmentally consistent dataset. Geometry-based removal remains dependent on the shape and consistency of processed foreground objects, and therefore dependent on the consistency of whichever foreground extractor is used for a scene. This dependency is not related to the properties of shadows within an environment, therefore we do not pursue further analysis with Geometry removal. SRT removal is based upon a series of Gabor filters used to characterize textural patches found in shadows, and match them to a corresponding background model. This technique requires that shadows cover textural portions of the background model large enough to perform meaningful analysis. The algorithm carries with it no meaningful contingencies for this limitation. The parameters associated with SRT removal define little avenue for improvement of shadow removal in most cases, and defines no avenue for cases mentioned where the shadows are too small. This dependence on a shadow's area, coupled with no parameters suitable to improve detection, eliminated SRT removal from candidacy. 

Both Geometry removal and SRT removal were shown to behave highly erratically from frame to frame even within an environmentally consistent dataset, demonstrated in Figure \ref{fig:geosrterratic}. Geometry-based removal remains dependent on the shape and consistency of processed foreground objects, and therefore dependent on the consistency of whichever foreground extractor is used for a scene. Furthermore, the algorithmic parameters associated with Geometry removal apply only to scenes in which the previous dependency is already fulfilled, i.e., the tunable parameters are relevant only to the geometric shapes necessary to enable Geometry removal. These dependencies are not related to the properties of shadows within an environment, therefore we do not pursue further analysis with Geometry removal. SRT removal is based upon a series of Gabor filters used to characterize textural patches found in shadows, and match them to a corresponding background model. This technique requires that shadows cover textural portions of the background model large enough to perform meaningful analysis. Manual tuning of SRT's algorithmic parameters provides no benefit to the algorithms detection or discrimination rates. This dependence on a shadow's area, coupled with the lack of parameters suitable to improve shadow removal, eliminated SRT removal from candidacy. 

\begin{figure}
\centering
\begin{subfigure}{.8\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_geo_erratic.jpg}
  \caption{Geometry}
\end{subfigure}
\hfill
\begin{subfigure}{.8\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_srt_erratic.jpg}
  \caption{Small-Region Texture}
\end{subfigure}

\caption{(a) and (b) showcase erratic shadow detection run on the dataset aton\_campus, which experiences no significant illumination change.}
\label{fig:geosrterratic}
\end{figure}

%Sanin et al.'s Large-Region Texture algorithm was also eliminated from the assessment due to its temperamental inter-frame performance, similar to the methods above. Some environments, such as the roadways of aton\_highway3, yielded null detections for all frames \textbf{NOTE: This specific test needs to be rerun, I know why the NULLs happened. Everything else is fine though}. The Large-Region Texture algorithm recognizes the pitfalls of the Small-Region approach, such as overly-small texture regions, and attempts to correct them using hard-coded parameters. Unfortunately, the algorithm as a whole displays an overly large sensitivity to change, both inter and intra-dataset (see Figure \ref{fig:lrt_sensitivity}). In a larger scoped study, one may approach this algorithm as an ideal candidate for potential improvement, given its large swath of tunable parameters and variable responses. However, for those reasons it was excluded from the remainder of this study. By selecting an algorithm with more controlled responses to change of environment, we can extend insights gleaned into improvements for other algorithms, including this one.

Sanin et al.'s Large-Region Texture algorithm was also eliminated from the assessment due to its inconsistent shadow removal, similar to Geometry and SRT removal (Figure \ref{fig:lrt_sensitivity}). Some environments, such as the roadways of aton\_highway1, yielded null detections for all frames. The Large-Region Texture algorithm recognizes the pitfalls of the Small-Region approach, such as restrictive texture regions, and attempts to correct them using hard-coded parameters. The algorithm primarily displays a sensitivity to environment, as identical parameter values produce vastly disparate shadow removal performances (Figure \ref{fig:lrt_sensitivity}). 

\begin{figure}
\centering
\begin{subfigure}{.8\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/lrt_sensitivity_2.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.8\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/lrt_sensitivity_1.jpg}
  \caption{}
\end{subfigure}
\caption{(a) Run on aton\_hallway, LRT shows consistent shadow removal accuracy, with occasional dips. (b) Run on aton\_campus, LRT performs erratically.}
\label{fig:lrt_sensitivity}
\end{figure}

LRT also demonstrates varying levels of parametric sensitivity, also dependent on the deployment environment. For example, the parameter \textit{avgAttenThresh} is highly sensitive, shown in Figure \ref{fig:avgattenthresh_response}, and has only one value per frame that affects detection and discrimination. Furthermore, shadow removal is affected disproportionately to \textit{avgAttenThresh}, with shadow detection rates often dropping to zero when affected. LRT has several parameters that behave in similar ways, demonstrating a lack of scalability for changing environmental properties.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/highway1_avgAttenThresh_response.jpg}

\caption{The LRT parameter \textit{avgAttenThresh} is highly sensitive, demonstrating a narrow range for which LRT removal accuracy is affected. LRT's removal accuracy response is shown, using the aton\_highway1 dataset.}
\label{fig:avgattenthresh_response}
\end{figure}

%Finally, both Chromacity-based and Physical-based shadow removal held the qualities ascribed to a suitable algorithm for exploration. In both cases, trends are observed, consistency of detection is within reasonable range, and the modification of parameters produces a scalable and easily observable response. This is visualized in Figure \ref{fig:similarities}.

\begin{figure}
\centering
  \includegraphics[width=.8\linewidth]{figures/similarities_chromacity_physical_hallway.jpg}

\caption{Chromacity and Physical shadow removal demonstrate similar shadow detection results run on aton\_hallway.}
\label{fig:similarities}
\end{figure}

Chromacity and Physical shadow removal provide tunable parameters that lie within acceptable ranges of sensitivity. An example of an acceptable range of sensitivity is seen in Figure \ref{fig:paramsensitivity}(b). The shadow removal algorithms often respond similarly to the diversity of environments in this study; for example, Figure \ref{fig:similarities} illustrates comparable detection rates for the aton\_hallway dataset. Chromacity and Physical shadow removal also display consistency between datasets, i.e., none of the datasets provide particularly poor results that are not linked to an environmental property change, such as illumination change. The Chromacity removal algorithm succeeds due to the tendency of shadows (within a dataset) to remain a consistent darkness when compared to the corresponding background model. However, this simple approach can prove problematic when illumination changes occur within a dataset. Physical shadow removal utilizes a similar system of gauging a shadow's darkness compared to a background model - what is referred to as a weak detector \cite{cucchiara2003detecting, sanin2010improved, huang2009moving} - and implicitly suffers similar breakdowns of detection when presented with significant illumination flux within a dataset. Although they both suffer, Physical shadow removal employs a strong detector to increase detection and discrimination at the cost of processing time. In the end, Physical shadow removal was selected as the main experimental framework for this study, in part due to Physical shadow removal's more sophisticated model.

\FloatBarrier
\subsection{Selecting a Parameter of Physical Shadow Removal} \label{section:selectparameter}

Physical shadow removal operates in two stages: the weak detector, and the strong detector. The weak detector typically identifies candidate shadow pixels simply by eliminating impossible pixels, i.e., pixels that are brighter than the background model. The remaining foreground pixels are then provided as candidate pixels to the strong detector, which characterizes these pixels as either shadow or foreground. Our assessment of algorithmic parameters explores parameters in both of these detector spaces. Before we can illustrate which parameters were considered for experimentation, we must first understand each parameter's place within the algorithm.

\subsubsection{Weak Detector - Physical Shadow Removal}

The purpose of the weak detector is to prevent impossible pixels from being fed to the strong detector. The weak detector in Physical shadow removal functions similarly to the Chromacity shadow removal process; the candidate pixel is evaluated by its distance from a corresponding background pixel and sorted accordingly. The weak detector is visually, and technically, a cone projected in an RGB plane indicating the range in which a normalized shadow pixel may lie with respect to a background model. Normalizing the shadow pixel's position relative to the background model is accomplished by computing the angular distance between the RGB vector of the foreground and the RGB vector of the background. The resultant represents the color-space deviation from foreground to background. This threshold parameter is named \textit{coneAngle} within the Physical shadow removal algorithm. The cone represents two dimensions: color deviation ($\theta$), and intensity (or brightness) deviation. A foreground pixel is considered a possible shadow if the normalized ratio of foreground to background brightness (scaled by a function of the color deviation ($cos(\theta)$), is within a pre-defined brightness range. The tolerable brightness range, delimited by parameters named \textit{coneR1} (minimum) and \textit{coneR2} (maximum), is hard-coded within the original algorithm to be 0.3 to 1.0. 
%\cite{huang2009moving} illustrates the conic model in Figure \ref{fig:cone_physical}.

%\begin{figure}
%  \centering
%  \includegraphics[width=.8\linewidth]{figures/cone_physical.jpg}
%  \caption{The conical volume describing the tolerable brightness range of a shadow pixel (\textit{SD}), in relation to a its illuminated background pixel (\textit{BG}). The maximal allowable color deviation, $\theta_{max}$, is represented in the algorithm as \textit{coneAngle}. Figure from \cite{huang2009moving}.}
%  \label{fig:cone_physical}
%\end{figure}

The main components of the weak detector are parameterized within the algorithm to fit a general model; \textit{coneR1} $<$ $x$ $<$ \textit{coneR2} as an intensity range works reasonably well for many scenes, but there exists an optimal range for any given environment. Similarly, the optimal color angle ($\theta$) between foreground and background vectors is dependent on environmental parameters. These parameters, \textit{coneR1}, \textit{coneR2}, and \textit{coneAngle} are therefore considered when looking for correlations with environmental conditions.

\FloatBarrier
\subsubsection{Strong Detector - Physical Shadow Removal}

After the weak detector removes impossible candidates for shadow pixels, a Gaussian Mixture Model (GMM) is used to learn the color features of shadow pixels when compared to background pixels. Using the remaining pixels, the GMM estimates the normalized spectral ratio for shadows in a scene, i.e., the ratio of spectral illuminants \cite{huang2009moving, sato2015foreground, lee2017shadow}. More information on spectral illuminants can be found in section \ref{section:nonlinearatten}. The GMM is a framework based upon Expectation Maximization and learning over time; therefore, the initial parameters presented in the algorithm have little influence over the efficacy of shadow removal. Shadow removal displayed sensitivity to only one significant parameter, \textit{postThresh}, the parameter controlling the posterior threshold. The posterior threshold is the threshold governing shadow/foreground assignment after the posterior probability of a pixel is determined via the GMM. Since the GMM adapts to its environment, modifying the posterior threshold invalidates any learning the GMM has achieved. Therefore, correlating \textit{postThresh} to environmental properties is not performed in this study.

\subsubsection{Evaluation of Parameters}

Due to their integral nature to both the weak and strong detectors, the parameters \textit{coneR1} and \textit{coneAngle} are evaluated for their effect on shadow removal. Each parameter is shown to have a pronounced effect on shadow removal. 

\begin{figure}
  \begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/pets1_coneR1_response.jpg}
  \caption{PETS1}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/pets2_coneR1_response.jpg}
  \caption{PETS2}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_coneR1_response.jpg}
  \caption{aton\_highway1}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway3_coneR1_response.jpg}
  \caption{aton\_highway3}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/room_coneR1_response.jpg}
  \caption{aton\_room}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_coneR1_response.jpg}
  \caption{aton\_campus}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/hallway_coneR1_response.jpg}
  \caption{aton\_hallway}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_coneR1_response.jpg}
  \caption{aton\_lab}
\end{subfigure}

\caption{Detection (blue) and discrimination (orange) rates are calculated as the value of \textit{coneR1} is varied from [0.0 .. 1.0]. Full results found in appendices.}
\label{fig:coneR1_iterate}
\end{figure}

\begin{figure}
  \begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/pets1_coneR1_score.jpg}
  \caption{PETS1}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/pets2_coneR1_score.jpg}
  \caption{PETS2}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_coneR1_score.jpg}
  \caption{aton\_highway1}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway3_coneR1_score.jpg}
  \caption{aton\_highway3}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/room_coneR1_score.jpg}
  \caption{aton\_room}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_coneR1_score.jpg}
  \caption{aton\_campus}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/hallway_coneR1_score.jpg}
  \caption{aton\_hallway}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_coneR1_score.jpg}
  \caption{aton\_lab}
\end{subfigure}

\caption{Example score ($\Xi$) of a frame for each dataset. Full results found in appendices.}
\label{fig:coneR1_iterate_score}
\end{figure}

Figure \ref{fig:coneR1_iterate} demonstrates \textit{coneR1}'s contribution to detection/discrimination across datasets. \textit{coneR1}'s effect on shadow removal remains consistent and scalable. The parameter is exercised through a large range, with an equally large range of resultant detection and discrimination rates. Figure \ref{fig:coneR1_iterate_score} shows that each dataset demonstrates an unambiguous maximum score, \textit{coneR1}*. The wider range and relatively coarse-grained nature of the parameter facilitates identifying trends and correlations with the iterations.

\FloatBarrier
\textit{coneR2} represents the upper limit of the cone describing a shadow pixel's color representation. While it serves a similar function as \textit{coneR1}, upon sensitivity testing, \textit{coneR2} was never found to contribute to higher detection and discrimination rates. Its default value (1.0) is also its optimal value (\textit{coneR2}*). Figure \ref{fig:coneR2_iterate} shows that the maximum of $\Xi$\textit{(coneR2)}, \textit{coneR2}*, is 1.0 across multiple datasets.

\begin{figure}
  \begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_coneR2_response.jpg}
  \caption{aton\_campus ($\eta$, $\xi$)}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/campus_coneR2_score.jpg}
  \caption{aton\_campus ($\Xi$)}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_coneR2_response.jpg}
  \caption{aton\_highway1 ($\eta$, $\xi$)}
\end{subfigure}
\hfill
  \begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/highway1_coneR2_score.jpg}
  \caption{aton\_highway1 ($\Xi$)}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/room_coneR2_response.jpg}
  \caption{aton\_room ($\eta$, $\xi$)}
\end{subfigure}
\hfill
\begin{subfigure}{.45\linewidth}
  \includegraphics[width=1\linewidth]{figures/room_coneR2_score.jpg}
  \caption{aton\_room ($\Xi$)}
\end{subfigure}

\caption{Parameter value responses for \textit{coneR2} for three datasets (aton\_campus, aton\_highway1, and aton\_room). Full results found in appendices.}
\label{fig:coneR2_iterate}
\end{figure}

\textit{coneAngle} inhabits too narrow a range of sensitivity to properly exploit (see results included in the appendices). Figure \ref{fig:coneAngle_iterate} illustrates the sensitivity of the parameter. Within the frames of a dataset, the calculated \textit{coneAngle}* values are within millionths of one another. Furthermore, \textit{coneAngle}* does not provide as large an improvement in shadow removal as \textit{coneR1}* does. The minimal removal improvement and narrow range of sensitive values makes further examination of \textit{coneAngle} difficult. For the duration of this study, environmental feature correlations were sought only in relation to \textit{coneR1}.

\begin{figure}
  \centering
  \begin{subfigure}{.8\linewidth}
  \includegraphics[width=1\linewidth]{figures/sensitive_param.jpg}
  \caption{}
\end{subfigure}
\hfill
\begin{subfigure}{.8\linewidth}
  \includegraphics[width=1\linewidth]{figures/sensitive_param_score.jpg}
  \caption{}
\end{subfigure}

\caption{(a) Shadow removal reponse (detection and discrimination) for \textit{coneAngle}, for aton\_highway1. (b) Shadow removal score for aton\_highway1. \textit{coneAngle}* is the maximum of the score.}
\label{fig:coneAngle_iterate}
\end{figure}

\FloatBarrier
\section{Assessment of Environmental Properties} \label{section:envassess}

We now create a proof-of-concept of our adaptive model, by correlating environmental properties with a previously assessed parameter \textit{coneR1}. We explore observed environmental properties within the datasets: attenuation and saturation. We also perform sensitivity analysis on multiple environmental properties (brightness functions, SIFT parameters) to explore their possible influence on correlation between \textit{coneR1}* and attenuation ($\alpha$). A model is given for using the observed environmental properties to calculate a new variable \textit{coneR1}$'$, which represents the adaptation of the \textit{coneR1} parameter to more closely match the curated \textit{coneR1}* ideal value.

\subsection{Previous Work - Large Region Texture Removal} \label{section:prevworkLRT}

Of all the leading shadow removal algorithms we evaluated, only one attempts to adapt its parameters to its current environment. In Sanin et al.'s LRT shadow removal \cite{sanin2010improved}, there are three examples of ambient environment properties taken into account: average saturation (of foreground pixels), average attenuation (from foreground to background), and average perimeter size of foreground objects. These three global properties govern the value of certain algorithm parameters.

LRT removal checks the average perimeter of foreground objects in a frame against a predetermined threshold. Average perimeter is linked to three additional algorithm parameters that control the size of the texture region being matched to identify shadows. If the average perimeter is above the predetermined threshold, the range of area for foreground/background correlation is expanded. The average perimeter is computed on a per-frame basis, while the threshold remains static.

LRT removal, like Physical shadow removal, uses a weak detector to retrieve candidate (shadow) pixels. The candidate shadow pixels must fit within a certain range of Hue (H), Saturation (S), and Value (V)(or Brightness) when compared against the background model, i.e., the foreground HSV values subtracted from the corresponding background HSV values, referred to as $\Delta$HSV, must fall within a specified range. There are two definitions of this acceptable range: $\Delta HSV_{1}\in$\{[0, 76], [0, 36], [0.6, 1.0]\}, and $\Delta HSV_{2}\in$\{[0, 62], [0, 93], [0.21, 0.99]\}. If the average saturation of all identified foreground pixels exceeds a predetermined threshold (average saturation = 35), $\Delta HS_{2}$ ($\Delta H$=[0, 62], $\Delta S$=[0, 93]) is used as the acceptable range. Similarly, if the average attenuation of the foreground pixels (compared to corresponding background pixels) exceeds its threshold (average attenuation = 1.58), the latter range of $\Delta V_{2}$ ([0.21, 0.99]) is used for evaluating brightness. 

This coarse-grained approach enables the LRT algorithm to switch between two presets for multiple parameters, based on observed environmental properties. However, the two presets ($\Delta HSV_{1}$, $\Delta HSV_{2}$), and the thresholds that govern them, require empirical knowledge about the scenes in which they are deployed. The preset-switching fails to adapt to environmental properties in a scalable manner. Applied to a 24-hour video feed, the preset-switching model certainly fares better than the naive fully hard-coded approach, but compromises shadow removal with its rigidity. We seek to adapt parameter values on a per-frame basis, with the scalability necessary to create a continuous function of environmental properties to parameter values.


%\begin{figure}
%  \centering
%  \includegraphics[width=.7\linewidth]{figures/mockup_cone_lrt.jpg}
%  \caption{Visualization of the volume of constraint imposed by LRT, and how that volume changes with scene parameters \textit{average attenuation} and \textit{average saturation}.}
%  \label{fig:cone_lrt}
%\end{figure}

This need for adaptability underscores the motivation of this research to discover a scalable and portable solution for parameterization. In the case of Physical shadow removal, foreground object perimeter has no bearing on the accuracy of shadow removal, because Physical removal utilizes a wholly pixel-based approach. However, both average saturation of foreground objects and average attenuation are examined further in this study, as described in the next section.

\subsection{Attenuation and Saturation}

 Attenuation ($\alpha$) represents the loss of intensity from foreground pixel to background pixel in a frame. Traditionally, brightness attenuation is represented mathematically in decibels (dB) as the ratio of background and foreground brightness (Eqn. \ref{eqn:db}). The $brightness()$ function used here is the HSV brightness function (Eqn. \ref{eqn:hsv}), which defines brightness as the maximum value between the R,G,B channels of a pixel. 

\begin{equation}
\alpha_{dB} = \dfrac{brightness(\vec{bg}(p))}{brightness(\vec{fg}(p))}
\label{eqn:db}
\end{equation}

\textit{p} represents pixel coordinates. In an effort to better capture shadow pixels, we discard any pixel where $\alpha < 1.0$, implying it is brighter than its corresponding background. While the $\alpha_{dB}$ representation adheres most closely to the physical definition of light attenuation within a shadow, we consider the reciprocal of this model (1/$\alpha_{dB}$) of attenuation for correlation. As all considered pixels have an $\alpha_{dB}$ of at least 1.0, the reciprocal bounds the value between 0.0 and 1.0. This range is more suitable for correlation calculations, as the covariance between data points is easily affected by large changes in value. More information regarding correlation is found in section \ref{section:corrofparams}.

Alternatively, we consider the \textit{percentage change} (\%$\Delta$) model of light attenuation. This model is defined in Eqn. \ref{eqn:percentchange}.

\begin{equation}
\alpha_{\%\Delta} = \dfrac{brightness(\vec{bg}(p) - \vec{fg}(p))}{brightness(\vec{bg}(p))}
\label{eqn:percentchange}
\end{equation}

$(1 - \alpha_{\%\Delta}$) is considered for correlation, which indicates the percentage of a background pixel's intensity the foreground represents.

Saturation is the measurement of ``depth of hue'' in a pixel, e.g., a lower saturation means the base hue is less expressed, while a higher saturation means the color of a pixel more closely matches the base hue. The saturation level of a background pixel affects the intensity attenuation a shadow pixel experiences. Saturation properties are discussed in greater detail in section \ref{section:lowcSIFT}.

\subsection{Non-linear Attenuation and Spectral Properties of Light} \label{section:nonlinearatten}

In the ideal case, the attenuation of light is an entirely linear function, i.e., a shadow pixel lies along the the vector drawn from the background pixel to the origin within the three dimensional RGB space \cite{huang2009moving}. Figure \ref{fig:rgbcube} illustrates attenuation of a pixel linearly and non-linearly.

\begin{figure}
  \centering
  \begin{subfigure}{.49\linewidth}
  	\includegraphics[width=1\linewidth]{figures/rgb_linear_atten.jpg}
  	\caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.49\linewidth}
  	\includegraphics[width=1\linewidth]{figures/rgb_nonlinear_atten.jpg}
  	\caption{}
  \end{subfigure}
 
\caption{(a) Shadow pixel \textit{SD} is attenuated linearly from background pixel \textit{BG}. (b) \textit{SD} is attenuated non-linearly.}
  \label{fig:rgbcube}
\end{figure}

Due to physical properties of illumination and reflectance, this linearity does not always hold true; cast shadows in real-world scenes are caused by more than one illumination source. In an outdoor scene, a shadow may be the product of direct sunlight, blue light refracted from the sky, and diffuse light scattering from nearby objects or surfaces. Any of these factors contribute to color bleed \cite{huang2009moving}, influencing the attenuation of shadow pixels. These disparate light sources are said to have different \textit{spectral power distributions}, different illumination characterized by varying concentrations of constituent wavelengths. This non-linear attenuation model is the primary motivator behind using GMM to learn a shadow's color model in \cite{huang2009moving}.

\FloatBarrier
\subsubsection{Observed Spectral Properties in Outdoor Scenes} \label{section:spectralprop}

Since the spectral properties of an illumination source cannot always be predicted, this study attempts to observe trends across datasets to properly characterize shadows. By visualizing the magnitude of color shift due to shadows, we can clearly partition datasets into outdoor and indoor images. Outdoor datasets experience change disproportionately in each channel of an RGB image, while indoor datasets are characterized by equal and predictable shifts in each channel.

\begin{figure}
  \centering
  \begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_pets1.jpg}
  \caption{PETS1}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_pets2.jpg}
  \caption{PETS2}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_highway1.jpg}
  \caption{aton\_highway1}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_highway3.jpg}
  \caption{aton\_highway3}
\end{subfigure}
\hfill
\begin{subfigure}{.7\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_campus.jpg}
  \caption{aton\_campus}
\end{subfigure}

\caption{Outdoor datasets demonstrate consistently greater deviations in the red and green channels. During illumination changes (evident in PETS1 and PETS2), the red and green channels shift disproportionate to that of the blue channel, indicating scattered blue light is a primary component of the shadows' spectral illuminant ratio.}
\label{fig:rgshift_outdoor}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_room.jpg}
  \caption{aton\_room}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_hallway.jpg}
  \caption{aton\_hallway}
\end{subfigure}
\hfill
\begin{subfigure}{.7\linewidth}
  \includegraphics[width=1\linewidth]{figures/rgshift_lab.jpg}
  \caption{aton\_lab}
\end{subfigure}

\caption{Indoor datasets demonstrate closer grouping of each channel's color shift. aton\_hallway behaves most erratically of the indoor datasets. This is due to color-bleed from nearby objects, making aton\_hallway the most diverse spectral ratio of the indoor sets. aton\_lab behaves linearly, as expected.}
\label{fig:rgshift_indoor}
\end{figure}

This disproportionate discrepancy is most apparent during periods of large illumination change, found prominently in datasets PETS1 and PETS2. Examining the outdoor datasets, we can see that the red and green channels experience larger magnitude shifts than in the blue channel. Based on assumed spectral power distributions of the source illumination found in the outdoor scenes, we can deduce that outdoor scenes display an introduction of blue light scattered from the sky that is not present in indoor scenes. With a spectral power distribution skewed towards blue, the red and green channels experience greater perturbation than the blue channel due to cast shadows. This study attempts to utilize this observed multi-illuminant model to better characterize the attenuation model assumed by a scene. 

\FloatBarrier
\subsection{Brightness Models} \label{section:brightnessmodels}

Since attenuation is a function of brightness, we attempt to utilize multiple methods of calculating brightness to improve attenuation modeling. We use different brightness functions in the calculation of $\alpha$ for each dataset, hypothesizing that correlation between $\alpha$ and \textit{coneR1}* can be improved using different brightness functions. We conduct a sensitivity analysis to determine the effect that different brightness models can have on attenuation.

Below is a short taxonomy of popular brightness models studied. Each of the brightness models included are tested and analyzed for its affect on the $\alpha$ model.

\subsubsection{HSV}

Described as a \lq{hexcone}\rq model, \cite{smith1978color}, brightness in an HSV representation is defined as the maximum value of the pixel represented in the red, green, and blue channels ($\vec{RGB}(p)$).

\begin{equation}
V = max(R, G, B)
\label{eqn:hsv}
\end{equation}

HSV can lead to misrepresentation of brightness as it is perceived by the human eye. This is because, as noted in Rec. 709/601 \cite{bt709, bt601}, the same intensity of green appears brighter than that of red, which, in turn, appears brighter than the same intensity of blue. In the HSV scheme, green at full intensity ($\hat{RGB} = (0, 1, 0)$) matches the brightness of blue at full intensity ($\hat{RGB} = (0, 0, 1)$). It also minimizes contributions from channels other than the dominant channel, e.g., $\hat{RGB}(0, 1, 0)$ = $\hat{RGB}(.9, 1, .9)$. HSV therefore is best suited for environments characterized by abnormally low saturation, where light attenuation due to shadows is most linear. Environments most likely to benefit from using HSV are scenes with a single illuminant source, such as indoor scenes.

\subsubsection{HSI}

HSI represents the most basic understanding of brightness, as \textit{Intensity}. 

\begin{equation}
I = \dfrac{R + G + B}{3}
\end{equation}

This understanding of brightness serves most environments properly, as it caters to each channel equally. However, HSI still suffers from the inherent luminance of certain colors, and fails to compensate for them.

\subsubsection{HSL}

HSL, a brightness representation called \textit{Lightness}, is called a \lq{bi-hexcone}\rq model \cite{smith1978color}. Lightness is an average of the primary and tertiary color components:

\begin{equation}
L = \dfrac{max(R,G,B) + min(R,G,B)}{2}
\end{equation}

Excluding the secondary color component has the effect of translating the brightness plane described by HSI. While perceptually similar to other brightness models, HSL provides more balanced values when one channel's value is in extrema.

\subsubsection{Relative Luminance (Y)}

Originally issued in 1982, Rec. 601 \cite{bt601} defines one of the first standards for converting analog signal into digital video. \textit{Relative Luminance (Y)}, or, when gamma-corrected, \textit{Luma (Y')} is the simplest extrapolation of perceptually-relevant brightness. Luminance is defined as a coefficient-weighted linear combination:

\begin{equation}
Y = 0.299R + 0.587G + 0.114B
\end{equation}

Rec. 709 later modified the coefficients to 0.21, 0.72, and 0.07, respectively. For this study's experimental purposes, Rec. 601 coefficients were used. This brightness model highlights the human eye's sensitivity to green hues, and is therefore particularly relevant when observing outdoor scenes. 

\subsubsection{Euclidean Norm}

Taking the Euclidean norm of an $\vec{RGB}$ vector is measuring the three-dimensional distance from absolute black, $\vec{RGB} = (0,0,0)$. 

\begin{equation}
Norm = \sqrt{\Delta R^2 + \Delta G^2 + \Delta B^2}
\end{equation}

While the Euclidean norm does not weight the channels perceptually, as Luminance does, it does represent the most accurate way to determine the color difference between two $\vec{RGB}$ vectors, and therefore proves valuable when comparing foreground pixels to background pixels. 

\subsubsection{HSP}

HSP, or \textit{Perceived Brightness}, combines the three dimensional distance of Euclidean norm, and the perceptually weighted coefficients of Luminance. HSP was introduced by Darel Rex Finley in 2006 \cite{hsp2006}.

\begin{equation}
P = \sqrt{0.299\Delta R^2 + 0.587\Delta G^2 + 0.114\Delta B^2}
\end{equation}

Ideally, HSP provides the most accurate perceptually conscious brightness. Environments that experience large saturation shifts due to shadows benefit primarily, as both color distance and weighted coefficients are considered.

\subsection{Low-contrast SIFT Keypoints} \label{section:lowcSIFT}

A scene typically contains a large set of low-level textural features unique to it. These low-level features have been characterized and quantified in many ways, such as the SIFT, SURF, or FAST feature descriptors \cite{lowe1999object, bay2008speeded, rosten2006machine}. These feature descriptors are used for image recognition and retrieval, and are robust to varying scales, rotations, and translations. These algorithms are traditionally performed on intensity images, i.e., grayscale images. However, hue and saturation play a large role in scene characterization. Therefore, SIFT implementations were developed to incorporate color information. Popular implementations of a color-SIFT method are HSV-SIFT, RGB-SIFT, HueSIFT, and others \cite{bosch2008scene, geusebroek2001color, van2006boosting, hancolor, tuytelaars2008local, abdel2006csift}.

Intensity images, while discarding chromatic information, often retain structural and textural information, due to image gradients' invariant basis in intensity. We assume a cast shadow similarly has minimal impact on underlying structures and textures, due to the success of textural shadow removal methods. As a result, SIFT keypoints remain largely invariant between a frame and its background model in intensity images. Since cast shadows do not modify the underlying textural structure, they do not affect the detection of SIFT keypoints.

In a traditional understanding of shadow attenuation as a linear process, both the hue and saturation of a pixel remain constant, as the intensity attenuates in a shadow region. Since this study assumes a different, non-linear understanding of light attenuation, we sought for representative changes that shadows bring to the hue and saturation channels of an HSV image. By detecting and displaying SIFT keypoints in the saturation channel alone, we observe qualitatively small localized structure changes within these shadowed regions (Figure \ref{fig:sat_struct}).

\begin{figure}
  \centering
  \begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_0161.jpg}
  \caption{aton\_lab}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_gt_0161.jpg}
  \caption{aton\_lab ground truth}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_sat_bg_0161.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_sat_fg_0161.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_sat_bg_zoom_0161.jpg}
  \caption{Saturation channel (background)}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_sat_fg_zoom_0161.jpg}
  \caption{Saturation channel (frame)}
\end{subfigure}

\caption{The limited structural effects of cast shadows in the saturation channel (d).}
\label{fig:sat_struct}
\end{figure}

We attempt to characterize these localized changes in structure using HSV-SIFT, by detecting SIFT keypoints using only the saturation channels of both a frame and its corresponding background model. It is apparent that foreground objects introduce significant structural changes in any of the three channels,  therefore the collection of SIFT descriptors is limited to only those considered \textit{low contrast}. The SIFT algorithm operates in four major stages: scale-space extrema extraction, elimination of low-contrast keypoints, elimination of strong edge responses, and the assignment of orientations. Finally, low-contrast detections are eliminated due to their susceptibility to noise \cite{lowe1999object}. In our study, we instead preserve low-contrast SIFT keypoints, to characterize the small structural changes found in shadow regions. We build two matrices: the first containing keypoints of low and normal contrast (still excluding edge responses), and the second containing only normal contrast keypoints. We then extract low-contrast keypoints using an exclusive-or operation. The non-linearity of light attenuation, paired with previously observed structural changes, make shadowed regions in the saturation channel ripe regions for low-contrast SIFT keypoints. When drawn onto the source frame, we can see that low-contrast SIFT keypoints align loosely with shadowed regions (Figure \ref{fig:sat_lowc_kp}). 

\begin{figure}
  \centering
  \begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_0161.jpg}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_gt_0161.jpg}
\end{subfigure}
\hfill
  \begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_lowc_rgb_0161.jpg}
  \caption{Low-contrast SIFT keypoints detected using the intensity channel.}
\end{subfigure}
\hfill
\begin{subfigure}{.49\linewidth}
  \includegraphics[width=1\linewidth]{figures/lab_lowc_sat_0161.jpg}
  \caption{Low-contrast SIFT keypoints detected in the saturation channel.}
\end{subfigure}

\caption{Detecting low-contrast SIFT keypoints in the saturation channel more effectively captures structural changes than in the intensity channel, due to cast shadows.}
\label{fig:sat_lowc_kp}
\end{figure}

With shadows partially characterized by low-contrast SIFT keypoints, this study utilizes this insight by modeling the approximate proportion of shadows introduced in a given frame. The quantity of detected SIFT features in a frame $f$ is represented as $SIFT_{c}(f)$, where $c$ represents the contrast ratio parameter used.
%The resultant $SIFT_{\%C}(f)$ is the ratio of total SIFT features that are low contrast features. 
%The approximate ratio of shadow pixels introduced to non-shadows pixels in calculated for both the foreground and background frames, using the ratio found in Eqn. \ref{eqn:lowc}.}
The ratio of total low contrast SIFT features to normal SIFT features is calculated for a single frame in Eqn. \ref{eqn:lowc}.

\begin{equation}
SIFT_{\%C}(f) = 1 - \dfrac{SIFT_{0.04}(f)}{SIFT_{0.01}(f)}
\label{eqn:lowc}
\end{equation}

$SIFT_{0.04}$ represents features captured using the default contrast threshold (0.04), and $SIFT_{0.01}$ represents features captured using a lower contrast threshold. We then compare the $SIFT_{\%C}(f)$ ratios of the foreground and background frames, as defined in Eqn. \ref{eqn:ratioratio}.

\begin{equation}
SIFT_{fg/bg} = \dfrac{SIFT_{\%C}(fg)}{SIFT_{\%C}(bg)}
\label{eqn:ratioratio}
\end{equation}

The resultant $SIFT_{fg/bg}$ estimates the percentage change of the number of detected low contrast SIFT features from the background to the current frame. We assume a greater presence of low contrast SIFT features loosely indicates a greater quantity of shadows in a frame. In order to boost correlation between a parameter and a primary environmental property, $SIFT_{fg/bg}$ is multiplied against the value of an observed environmental property (in our case, $\alpha$), serving as a scaling factor. We use this scaling factor to perform sensitivity analysis using the parameter \textit{coneR1}* and the environmental property $\alpha$. Results of this analysis are found in section \ref{section:lowcsensitivity}.

\FloatBarrier
\section{Weak Detector Estimation - Creating a Model} \label{section:model}

With \textit{coneR1}* calculated for a wide variety of datasets, we can begin to model a difference between environmental properties and the optimal parameter.While the attenuation from the foreground to the background displays a correlative relationship to \textit{coneR1}*, this correlation does not indicate a relationship to the magnitude of \textit{coneR1}*. This is due to the nature of the properties of attenuation. The attenuation calculation utilized is a function of brightness shift as a percentage of a background value, e.g., a pair of foreground/background pixels with a value of 50 and 100 produces the same attenuation as a pair valued at 25 and 50. Normalizing correlative environmental properties, we model an optimal shift from observed properties to the optimal value of the weak detector, i.e., the shift is modeled as $shift_{req.} = (coneR1$*$ - \alpha$).

$shift_{req.}$ can be extrapolated from the magnitude of color shift ($\Delta$RGB) found from foreground to background. By plotting $\Delta$RGB against $shift_{req.}$, we produce a general relationship. Using data points from each frame in each dataset, we can produce a best-fit polynomial to generalize a model of attenuation and brightness magnitude shift into a function dictating the required shift of attenuation (shown in Figure \ref{fig:polyfit}). This required shift is defined as $shift_{\alpha}$. To avoid over-fitting, a parametrized logarithm was chosen to represent the required magnitude shift ($\Delta$RGB). The logarithm used to calculate $shift_{\alpha}$, derived from Figure \ref{fig:polyfit}, is defined in Eqn. \ref{eqn:logarithm}.

\begin{equation}
shift_{\alpha} = -0.11307*\ln(\Delta RGB) - 0.1884 
\label{eqn:logarithm}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{figures/model_hsv.jpg}
  \caption{Regression performed on RGB shift found per frame, vs. required magnitude shift to optimal.}
  \label{fig:polyfit}
\end{figure}

We are now able to calculate a new algorithmic parameter, \textit{coneR1}$'$. \textit{coneR1}$'$ is calculated per frame, based on $\alpha_{\%\Delta}$.
Calculating \textit{coneR1}$'$ is shown in Eqn. \ref{eqn:modelshift}.

\begin{equation}
coneR1' = (1 - \Delta RGB)*(1 - \alpha_{\%\Delta}) + shift_{\alpha}
\label{eqn:modelshift}
\end{equation}